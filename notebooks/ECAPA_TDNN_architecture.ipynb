{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqGjZaTXrxWHvcfRjbS0mR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01PrathamS/ECAPA-TDNN-Implementation/blob/main/notebooks/ECAPA_TDNN_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bhA86EFxKtg-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SEBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, channels, reduction=16):\n",
        "    super(SEBlock, self).__init__()\n",
        "    self.fc1 = nn.Linear(channels, channels // reduction)\n",
        "    self.fc2 = nn.Linear(channels // reduction, channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    scale = x.mean(dim=-1)\n",
        "    scale = F.relu(self.fc1(scale))\n",
        "    scale = torch.sigmoid(self.fc2(scale))\n",
        "    return x * scale.unsqueeze(-1)\n",
        "\n",
        "\n",
        "class Res2Block(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, scale=4):\n",
        "    super(Res2Block, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
        "    self.res2_layers = nn.ModuleList([\n",
        "        nn.Conv1d(out_channels // scale, out_channels // scale, kernel_size=3, padding=1)\n",
        "        for _ in range(scale - 1)\n",
        "    ])\n",
        "    self.conv3 = nn.Conv1d(out_channels, out_channels, kernel_size=1)\n",
        "    self.bn = nn.BatchNorm1d(out_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    split = torch.chunk(x, len(self.res2_layers) + 1, dim=1)\n",
        "    res_outs = [split[0]]\n",
        "    for i, conv in enumerate(self.res2_layers):\n",
        "      res_outs.append(conv(split[i + 1]) + res_outs[-1])\n",
        "    x = torch.cat(res_outs, dim=1)\n",
        "    x = self.conv3(x)\n",
        "    return self.relu(self.bn(x))\n",
        "\n",
        "class AttentiveStatisticsPooling(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(AttentiveStatisticsPooling, self).__init__()\n",
        "        self.attention = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_weights = torch.tanh(self.attention(x.permute(0, 2, 1)))\n",
        "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
        "        mean = torch.sum(x * attn_weights.permute(0, 2, 1), dim=-1)\n",
        "        std = torch.sqrt(torch.sum(x**2 * attn_weights.permute(0, 2, 1), dim=-1) - mean**2)\n",
        "        return torch.cat([mean, std], dim=1)"
      ],
      "metadata": {
        "id": "8GFvybGcKwxL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ECAPA_TDNN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim=30, emb_dim=512, num_speakers=1000):\n",
        "    super(ECAPA_TDNN, self).__init__()\n",
        "\n",
        "    self.layer1 = Res2Block(input_dim, 512)\n",
        "    self.layer2 = Res2Block(512, 512)\n",
        "    self.layer3 = Res2Block(512, 512)\n",
        "\n",
        "    # Multi layer feature aggregation\n",
        "    self.mfa = nn.Conv1d(3 * 512, 1536, kernel_size=1)\n",
        "\n",
        "    self.se_block = SEBlock(1536)\n",
        "\n",
        "    self.pooling = AttentiveStatisticsPooling(1536)\n",
        "\n",
        "    self.fc1 = nn.Linear(1536 * 2, emb_dim)\n",
        "    self.bn = nn.BatchNorm1d(emb_dim)\n",
        "\n",
        "    self.fc2 = nn.Linear(emb_dim, num_speakers)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = x.permute(0, 2, 1)\n",
        "\n",
        "    x1 = self.layer1(x)\n",
        "    x2 = self.layer2(x1)\n",
        "    x3 = self.layer3(x2)\n",
        "\n",
        "\n",
        "    x = torch.cat([x1, x2, x3], dim=1)\n",
        "    x = self.mfa(x)\n",
        "\n",
        "    x = self.se_block(x)\n",
        "\n",
        "    x = self.pooling(x)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = self.bn(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    logits = self.fc2(x)\n",
        "\n",
        "    return x, logits\n",
        "\n",
        "\n",
        "model = ECAPA_TDNN(input_dim=30, emb_dim=512, num_speakers=1000)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwVtGLVZMPVj",
        "outputId": "ee37df5f-04f5-4765-d192-40acc1f5875f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECAPA_TDNN(\n",
            "  (layer1): Res2Block(\n",
            "    (conv1): Conv1d(30, 512, kernel_size=(1,), stride=(1,))\n",
            "    (res2_layers): ModuleList(\n",
            "      (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    )\n",
            "    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer2): Res2Block(\n",
            "    (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
            "    (res2_layers): ModuleList(\n",
            "      (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    )\n",
            "    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (layer3): Res2Block(\n",
            "    (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
            "    (res2_layers): ModuleList(\n",
            "      (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    )\n",
            "    (conv3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
            "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (mfa): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
            "  (se_block): SEBlock(\n",
            "    (fc1): Linear(in_features=1536, out_features=96, bias=True)\n",
            "    (fc2): Linear(in_features=96, out_features=1536, bias=True)\n",
            "  )\n",
            "  (pooling): AttentiveStatisticsPooling(\n",
            "    (attention): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "  )\n",
            "  (fc1): Linear(in_features=3072, out_features=512, bias=True)\n",
            "  (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Training Example\n",
        "\n",
        "batch_size, time_steps, features = 16, 100, 30\n",
        "\n",
        "num_speakers = 1000\n",
        "\n",
        "dummy_data = torch.randn(batch_size, time_steps, features)\n",
        "dummy_labels = torch.randint(0, num_speakers, (batch_size,))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad()\n",
        "embedding, logits = model(dummy_data)\n",
        "loss = criterion(logits, dummy_labels)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "\n",
        "print(f\"Training Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-dPqXNmNos_",
        "outputId": "4bf70080-9eca-421b-83d4-a1f6062e621d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 6.8680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8gLQS2YTPJ73"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}